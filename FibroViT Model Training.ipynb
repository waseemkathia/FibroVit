{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc351c0",
   "metadata": {},
   "source": [
    "An interactive notebook that trains a Vision Transformer (ViT) model to classify images from a **local dataset stored in Google Drive**. It connects to your drive, loads the custom image dataset, and uses the `transformers` library for training and evaluation. The final model and its performance metrics are saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774d614",
   "metadata": {},
   "source": [
    "# ViT Model and Image Requirements\n",
    "\n",
    "### Hardware Note\n",
    "I have used Google Colab Pro with a GPU-based runtime, but you can also use the free version of Google Colab with a T4 GPU.\n",
    "\n",
    "---\n",
    "\n",
    "### Image Specifications\n",
    "\n",
    "**Image Size** :\n",
    "The `google/vit-base-patch16-224-in21k` model expects images with a resolution of 224x224 pixels. The `224` in the model's name specifies this requirement. The `ViTFeatureExtractor` used in the script automatically resizes your input images to this standard size during the preprocessing step.\n",
    "\n",
    "**Color Format** :\n",
    "The model requires images to be in the RGB color format, meaning they must have 3 color channels. The updated notebook includes a step (`ds.cast_column(\"image\", Image(decode=True, id=None))`) to ensure images are loaded and processed correctly as RGB. Grayscale images would need to be converted to RGB before being fed to the model.\n",
    "\n",
    "**Dimensionality** :\n",
    "Each RGB image is treated as a 3D data structure or tensor, with dimensions representing Height, Width, and Channels (e.g., 224 x 224 x 3). When these images are processed in batches, the model's input becomes a 4D tensor (batch size, channels, height, width)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a8e02",
   "metadata": {},
   "source": [
    "### 1. Install/Upgrade Libraries\n",
    "\n",
    "This cell ensures the necessary libraries (`datasets`, `evaluate`) are installed and up-to-date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02def4",
   "metadata": {},
   "source": [
    "### 2. Verify Library and Python Versions\n",
    "\n",
    "This cell confirms that the correct versions of the libraries and Python are loaded in the environment before running the main script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"Evaluate version: {evaluate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2bd57b",
   "metadata": {},
   "source": [
    "### 3. Mount Google Drive\n",
    "\n",
    "This cell mounts your Google Drive to the Colab environment. This is essential for accessing your local dataset and saving the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19de1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdf1c3",
   "metadata": {},
   "source": [
    "### 4. How to Structure Your Local Dataset\n",
    "\n",
    "For the script to work, you must organize your image files in the `ImageFolder` format and upload them to Google Drive. The structure should be as follows:\n",
    "\n",
    "```\n",
    "My_Dataset/               <-- Your main dataset folder\n",
    "├── train/                <-- Training images\n",
    "│   ├── Class_A/          <-- Folder for the first class\n",
    "│   │   ├── image1.jpg\n",
    "│   │   └── image2.png\n",
    "│   └── Class_B/          <-- Folder for the second class\n",
    "│       ├── image3.jpg\n",
    "│       └── ...\n",
    "├── validation/           <-- Validation images (same structure as train)\n",
    "│   ├── Class_A/\n",
    "│   └── Class_B/\n",
    "└── test/                 <-- Test images (same structure as train)\n",
    "    ├── Class_A/\n",
    "    └── Class_B/\n",
    "```\n",
    "\n",
    "Upload the main folder (e.g., `My_Dataset`) to your Google Drive and update the path in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5dc30c",
   "metadata": {},
   "source": [
    "### 5. The Complete Model Training and Evaluation Pipeline\n",
    "\n",
    "This is the main script that orchestrates the entire process. It now loads data from the Google Drive path you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9c1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Image\n",
    "import evaluate \n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from google.colab import drive\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the model training and evaluation pipeline.\n",
    "    \"\"\"\n",
    "    # --- Mount Google Drive ---\n",
    "    # This ensures the drive is connected.\n",
    "    print(\"Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "    # --- HARDWARE CHECK ---\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Hardware check: Using device = {device}\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"WARNING: Training on a CPU is very slow. Consider using a GPU runtime.\")\n",
    "\n",
    "    print(\"\\nStarting Phase 1: AI Model Training...\")\n",
    "\n",
    "    # 1. Load Your Local Dataset\n",
    "    # IMPORTANT: Update this path to point to your dataset folder in Google Drive.\n",
    "    local_dataset_path = \"/content/drive/MyDrive/My_Dataset\" \n",
    "\n",
    "    print(f\"\\nStep 1: Loading local dataset from '{local_dataset_path}'...\")\n",
    "    try:\n",
    "        ds = load_dataset('imagefolder', data_dir=local_dataset_path)\n",
    "        # The .cast_column method ensures images are loaded in RGB format, which is standard for ViT.\n",
    "        ds = ds.cast_column(\"image\", Image(decode=True, id=None))\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        print(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Please ensure the path is correct and the directory structure matches the 'ImageFolder' format.\")\n",
    "        return\n",
    "\n",
    "    # Extract labels for model configuration\n",
    "    labels = ds['train'].features['label'].names\n",
    "    label2id = {label: i for i, label in enumerate(labels)}\n",
    "    id2label = {i: label for i, label in enumerate(labels)}\n",
    "    print(f\"\\nClasses found: {labels}\")\n",
    "\n",
    "    # 2. Choose Your Framework & Preprocessing\n",
    "    print(\"\\nStep 2: Initializing Feature Extractor...\")\n",
    "    model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "    print(\"Feature extractor initialized.\")\n",
    "\n",
    "    # Create a transformation function\n",
    "    def transform(example_batch):\n",
    "        inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "        inputs['label'] = example_batch['label']\n",
    "        return inputs\n",
    "\n",
    "    print(\"Applying transformation to the dataset...\")\n",
    "    prepared_ds = ds.with_transform(transform)\n",
    "    print(\"Transformation complete.\")\n",
    "\n",
    "    # 3. Define Model and Training Script\n",
    "    print(\"\\nStep 3: Defining Model Architecture (ViT)...\")\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=len(labels),\n",
    "        id2label={str(i): c for i, c in enumerate(labels)},\n",
    "        label2id={c: str(i) for i, c in enumerate(labels)}\n",
    "    )\n",
    "    print(\"Model defined.\")\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/content/drive/MyDrive/my-pulmonary-fibrosis-vit-local\",\n",
    "        per_device_train_batch_size=16,\n",
    "        eval_strategy=\"steps\",\n",
    "        num_train_epochs=1, # Set to a higher number (e.g., 10) for full training\n",
    "        fp16=True if device == 'cuda' else False,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        logging_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        report_to='tensorboard',\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Define metrics computation\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    def compute_metrics(p):\n",
    "        predictions = np.argmax(p.predictions, axis=1)\n",
    "        return accuracy_metric.compute(predictions=predictions, references=p.label_ids)\n",
    "\n",
    "    # Define data collator\n",
    "    def collate_fn(batch):\n",
    "        return {\n",
    "            'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "            'labels': torch.tensor([x['label'] for x in batch])\n",
    "        }\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=prepared_ds[\"train\"],\n",
    "        eval_dataset=prepared_ds[\"validation\"],\n",
    "        tokenizer=feature_extractor,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    train_results = trainer.train()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # 4. Save the Trained Model\n",
    "    print(\"\\nStep 4: Saving the final model...\")\n",
    "    trainer.save_model()\n",
    "    trainer.log_metrics(\"train\", train_results.metrics)\n",
    "    trainer.save_metrics(\"train\", train_results.metrics)\n",
    "    trainer.save_state()\n",
    "    print(f\"Model saved successfully to '{training_args.output_dir}'\")\n",
    "\n",
    "    # --- Evaluation on Test Set ---\n",
    "    print(\"\\n--- Starting Evaluation on Test Set ---\")\n",
    "    test_dataset = prepared_ds[\"test\"]\n",
    "    predictions_output = trainer.predict(test_dataset)\n",
    "    y_true = np.array(test_dataset.with_format(\"torch\")[:]['label'])\n",
    "    y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
    "\n",
    "    # Assuming binary classification for these metrics\n",
    "    # For multi-class, you may need to adjust the 'average' parameter\n",
    "    if len(labels) == 2:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        try:\n",
    "            auc_roc = roc_auc_score(y_true, y_pred)\n",
    "            auc_pr = average_precision_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            auc_roc = float('nan')\n",
    "            auc_pr = float('nan')\n",
    "        print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "        print(f\"Test Set Precision: {precision:.4f}\")\n",
    "        print(f\"Test Set Recall: {recall:.4f}\")\n",
    "        print(f\"Test Set MCC: {mcc:.4f}\")\n",
    "        print(f\"Test Set AUC-ROC: {auc_roc:.4f}\")\n",
    "        print(f\"Test Set AUC-PR: {auc_pr:.4f}\")\n",
    "    \n",
    "    # Accuracy is always applicable\n",
    "    accuracy = accuracy_metric.compute(predictions=y_pred, references=y_true)[\"accuracy\"]\n",
    "    print(f\"\\nTest Set Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
